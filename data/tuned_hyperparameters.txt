learning_rate	|	0.059
decay_steps		|	115
decay_rate		|	0.92243
beta_1		|	0.92669
beta_2		|	0.99459
clipvalue		|	2.7365
network_depth	|	1
layer_units		|	15

	

hyperparameters = HyperParameter()

# Definition of all the hyperparameters that we want sample
hyperparameters.float("learning_rate",min_val=1e-3,max_val=7.5e-2)
hyperparameters.float("clipvalue",min_val=9e-1,max_val=4)
hyperparameters.float("beta_1",min_val=0.87,max_val=0.93)
hyperparameters.float("beta_2",min_val=0.98888,max_val=0.9999)
hyperparameters.float("decay_rate",min_val=0.88,max_val=0.98)
hyperparameters.int("decay_steps",min_val=80,max_val=120)
hyperparameters.boolean("layer_1")
hyperparameters.boolean("layer_2")
hyperparameters.selection("activation_1",selections=["elu","relu"])
hyperparameters.selection("activation_2",selections=["elu","relu"])
hyperparameters.selection("activation_3",selections=["elu","relu"])


hyperTmodel = netmods.HyperTModel(feature_ndims)

HPtuner = TuningScheduler(hyperparameters,hyperTmodel,X_train,Y_train,X_val,Y_val,callbacks)

num_random_configs = 150
num_grid_configs = 10
num_epochs = int(1e3)

#TODO: remove figurative from TuningScheduler and LossLogger

tuned_hparams = HPtuner.rand_tune(num_random_configs,num_epochs,batch_size,verbose=2,figurative=0,log_frequency=2)